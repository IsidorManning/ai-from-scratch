{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MIT License (MIT) Copyright (c) 2020 Andrej Karpathy\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person\n",
    "# obtaining a copy of this software and associated documentation\n",
    "# files (the \"Software\"), to deal in the Software without restriction,\n",
    "# including without limitation the rights to use, copy, modify, merge,\n",
    "# publish, distribute, sublicense, and/or sell copies of the Software,\n",
    "# and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be\n",
    "# included in all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
    "# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\n",
    "# OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n",
    "# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n",
    "# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n",
    "# OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\n",
    "# USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available?: True\n",
      "My GPU: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "# We will run our code on our GPU\n",
    "print('Is GPU available?:', torch.cuda.is_available()) \n",
    "print('My GPU:', torch.cuda.get_device_name()) # Name of my GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt_v1'\n",
    "\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 1115394\n",
      "Number of Unique Characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('../datasets/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "\ttext = f.read()\n",
    "\n",
    "num_characters = len(text)\n",
    "print('Length of dataset:', num_characters)\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print('Number of Unique Characters:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating The Character Mappings, Encoders, and Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch: i for i, ch in enumerate(chars) }\n",
    "itos = { i: ch for i, ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = encode(text)\n",
    "data = torch.tensor(encoded_data, dtype=torch.long)\n",
    "\n",
    "val_percentage = 0.1\n",
    "n = int((1 - val_percentage) * len(data)) \n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "\tdata = train_data if split == 'train' else val_data\n",
    "\tix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\t\n",
    "\tx = torch.stack([data[i: i + block_size] for i in ix])\n",
    "\ty = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "\t\n",
    "\tx, y = x.to(device), y.to(device)\n",
    "\t\n",
    "\treturn x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch_train, y_batch_train = get_batch('train')\n",
    "x_batch_val, y_batch_val = get_batch('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention In Neural Networks\n",
    "\n",
    "Self-attention is a mechanism in neural networks, particularly in Transformer models, that allows each token (or word) in a sequence to interact with and consider the relevance of other tokens in the same sequence. This process helps the model understand relationships between words and capture context more effectively. By weighing the importance of each word relative to others, self-attention enables the model to see the \"bigger picture,\" allowing it to grasp the overall meaning and nuances of the text.\n",
    "\n",
    "Notice that we do not want tokens to communicate with other tokens ahead of themselves (it wouldn't make sense to get information from the future...). Instead, they only want to be able to communicate with tokens that came before themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to achieve self attention is to simply sum the previous tokens up for any token, which implicitly captures the data and information of them. The term used for this in the context of GPTs is \"BOW\" which stands for \"Bag Of Words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize our 'x_bow' to zeros\n",
    "x_bow = torch.zeros((B, T, C))\n",
    "\n",
    "for batch in range(B):\n",
    "\tfor token in range(T):\n",
    "\t\t# Get all previous tokens from the start up\n",
    "\t\t# until the current token, including itself.\n",
    "\t\tx_prev = x[batch, :token + 1]\n",
    "\t\t# Calculate the mean of 'x_prev' across the\n",
    "\t\t# zeroth dimension which is 'T' since 'x_prev'\n",
    "\t\t# is of shape (T, C). Then, we store this in\n",
    "\t\t# the bag of words.\n",
    "\t\tx_bow[batch, token] = torch.mean(x_prev, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Matrix Multiplication\n",
    "\n",
    "The above implementation of BOW Self Attention is very brute force and not effecient. We can use matrix multiplication and triangular matrices to make this computation much more effective using the 'torch.tril()' function and the 'F.softmax()' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before softmax:\n",
      " tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf]])\n",
      "\n",
      "Weights after softmax:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000]])\n",
      "\n",
      "Weights shape: torch.Size([8, 8])\n",
      "x shape: torch.Size([4, 8, 2])\n",
      "x_bow_2 shape: torch.Size([4, 8, 2])\n",
      "Are x_bow and x_bow_2 the same?: True\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(T, T)\n",
    "tril = torch.tril(ones)\n",
    "# The 'weights' give the percentege of how much we essentially want to add up\n",
    "# when we later add up previous tokens using matrix multiplication with the \n",
    "# inputs, 'x'.\n",
    "weights = torch.zeros((T,T))\n",
    "\n",
    "# having the upper triangle matrix being -inf (and after the softmax\n",
    "# function, zero) allos us to mathematically tell the model not to retrieve \n",
    "# information and build context based on future tokens (those that lie after the\n",
    "# current token).\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "print('Weights before softmax:\\n', weights[:5, :])\n",
    "\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "print('\\nWeights after softmax:\\n', weights[:5, :])\n",
    "\n",
    "print('\\nWeights shape:', weights.shape)\n",
    "print('x shape:', x.shape)\n",
    "\n",
    "x_bow_2 = weights @ x \n",
    "print('x_bow_2 shape:', x_bow_2.shape)\n",
    "\n",
    "\n",
    "print('Are x_bow and x_bow_2 the same?:', torch.allclose(x_bow, x_bow_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Self Attention Head\n",
    "\n",
    "What we looked at above was a single \"head\" of self attention. We will now take this idea and build on top of it. \n",
    "\n",
    "In a self-attention, attention heads are used to allow the model to focus on different aspects of the input sequence simultaneously. Each head operates independently, creating its own set of 'key', 'query', and 'value' vectors by linearly transforming the input embeddings. \n",
    "* The 'key' represents the content of the tokens that the model will attend to, essentially acting as the information that other tokens will query. \n",
    "* The 'query' is a vector that interacts with the keys to determine how much attention one token should pay to another. \n",
    "* The 'value' carries the actual information from the tokens, which is then weighted by the attention scores and combined to produce the output for each token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 32 \n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n",
      "tensor([[ 0.2484,  0.0566,  0.8336,  0.3124, -0.0440,  0.0730, -0.0134,  0.0907,\n",
      "          1.0683, -0.4445, -0.1486,  0.1959,  0.1864, -0.3188,  0.1218,  0.0935],\n",
      "        [ 0.2273,  0.0474,  0.8315,  0.3182, -0.0072,  0.0839,  0.0108,  0.1599,\n",
      "          1.0144, -0.4256, -0.1185,  0.2179,  0.1797, -0.3102,  0.1051,  0.1473],\n",
      "        [-0.1394,  0.0411,  0.5235,  0.2729,  0.2268,  0.1124,  0.0875,  0.9478,\n",
      "         -0.0738, -0.1358, -0.0544,  0.2660,  0.0181, -0.2933,  0.1738,  0.5395],\n",
      "        [ 0.2276,  0.3147,  0.6287, -0.0114, -0.0069,  0.0644, -0.3685,  0.2278,\n",
      "         -0.0244,  0.1210, -0.3476, -0.2308,  0.0240, -0.1796,  0.0431,  0.0167],\n",
      "        [ 0.1132,  0.2551,  0.6288, -0.1467, -0.1872,  0.1291, -0.4193, -0.0381,\n",
      "         -0.1684,  0.1416, -0.2571, -0.0698, -0.0531, -0.2164, -0.0137,  0.0847],\n",
      "        [-0.0520, -0.0197,  0.7881,  0.3137,  0.5464,  0.2514,  0.2730,  1.1590,\n",
      "          0.0060,  0.0042,  0.2914,  0.4282,  0.0504, -0.1321, -0.2023,  0.9130],\n",
      "        [-0.2873, -0.0389,  0.2482, -0.0948, -0.5423,  0.3404,  0.2418, -0.3955,\n",
      "          0.4854, -0.1161, -0.2796,  0.2302,  0.4120, -0.1697,  0.3717,  0.1422],\n",
      "        [-0.2973, -0.0538,  0.3613, -0.1434, -0.6063,  0.2890,  0.0282, -0.4336,\n",
      "          0.1972, -0.1273, -0.1623,  0.3825,  0.1224, -0.3533,  0.2473,  0.2319]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Represents the dimensionality of the queries, keys, and values within \n",
    "# each attention head.\n",
    "head_size = 16\n",
    "\n",
    "# The following are three linear layers which we will use in self attention\n",
    "# heads. We perform a forward pass through these layers to aquire our keys,\n",
    "# queries, and values. All these layers are linear transformations that maps \n",
    "# the input of size 'C' to an output of size 'head_size'.\n",
    "key_layer = nn.Linear(C, head_size, bias=False)\n",
    "query_layer = nn.Linear(C, head_size, bias=False)\n",
    "value_layer = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# Perform the forward passes through the linear layers. Notice that the \n",
    "# keys, quires, and values do not communicate with eachother at this stage. \n",
    "key_layer = key_layer(x)\n",
    "query_layer = query_layer(x)\n",
    "value_layer = value_layer(x)\n",
    "\n",
    "# The following line is what \"applies the self attention\", allowing the 'query'\n",
    "# and 'key' to communicate with eachother. The transpose operation is necessary\n",
    "# to perform the matrix multiplication. This is how the dimensions work out:\n",
    "# (B, T, 16) @ (B,  16, T) = (B, T, T). This is the attention score matrix, \n",
    "# where each element (i, j) indicates how much focus the i-th token in the \n",
    "# sequence should have on the j-th token. This is how attention works and \n",
    "# how we encode the significance of the relationships between tokens in \n",
    "# the embedding space into the current token.\n",
    "weights =  query_layer @ key_layer.transpose(-2, -1)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "# Think of 'value' as the source of the information that gets redistributed \n",
    "# according to the attention 'weights'. While 'query' and 'key' help determine \n",
    "# where to look (which tokens to focus on), 'value' provides the what \n",
    "# (the actual content that will be passed along to the next layer, \n",
    "# informed by the attention head)\n",
    "out = weights @ value_layer\n",
    "\n",
    "print(out.shape)\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents one head of self-attention\n",
    "class Head(nn.Module):\n",
    "\t# Initilizes the self attention head with some set 'head_size'\n",
    "\tdef __init__(self, head_size):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t# Create the linear layers for the keys, queries, and values.\n",
    "\t\tself.key_layer = nn.Linear(n_embd, head_size, bias=False)\n",
    "\t\tself.query_layer = nn.Linear(n_embd, head_size, bias=False)\n",
    "\t\tself.value_layer = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "\t\t# Creates a 'tril' variable as a buffer using PyTorch.\n",
    "\t\tself.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "\t\t# Initilize the dropout\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# Get the dimensions\n",
    "\t\tB, T, C = x.shape\n",
    "\t\t\n",
    "\t\t# Feedforward through the layers to get 'key' and 'query'\n",
    "\t\tkey = self.key_layer(x)   # (B, T, C)\n",
    "\t\tquery = self.query_layer(x) # (B, T, C)\n",
    "\n",
    "\t\t# Allow the 'key' and 'query' to communicate with eachother\n",
    "\t\tweights = query @ key.transpose(-2, -1) * C ** -0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "\t\t\n",
    "\t\t# Set the upper triangle of the matrix to '-inf' to prevent the model\n",
    "\t\t# from learning from future tokens.\n",
    "\t\tweights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "\t\t\n",
    "\t\t# compute attention scores and store them in 'weights'\n",
    "\t\tweights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "\n",
    "\t\t# Apply dropout to the attention scores\n",
    "\t\tweights = self.dropout(weights)\n",
    "\n",
    "\t\t# perform the weighted aggregation of the values\n",
    "\t\tvalue = self.value_layer(x) # (B, T, C)\n",
    "\t\tout = weights @ value # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "\n",
    "\t\treturn out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention\n",
    "\n",
    "Simply put, multi head attention is simply stacking multiple self-attention blocks and concatenating the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents multiple heads of self-attention in parallel\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\t\n",
    "\t# Initilizes the multi head attention block with a set\n",
    "\t# number of heads, 'num_heads' and a set 'head_size'\n",
    "\tdef __init__(self, num_heads, head_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# Initilize 'num_heads' amounnt of self-attention heads and append them\n",
    "\t\t# to a type of list offered in PyTorch called a \"ModuleList\"\n",
    "\t\tself.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "\t\t# We will add something called Residual Connections to the neural network\n",
    "\t\t# where we will need to use a projection (linear transformation) to \n",
    "\t\t# project our output data from the neural network back to these \n",
    "\t\t# residual connections\n",
    "\t\tself.projection = nn.Linear(n_embd, n_embd)\n",
    "\t\t# Initilize the Dropout\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# For each self attention head in 'self.heads', call the module\n",
    "\t\t# and apply the self attention to the input data, 'x'. Then, \n",
    "\t\t# concatenate the results for all self attention heads along\n",
    "\t\t# the channel dimension (the last dimension).\n",
    "\t\tout = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\t\t# Apply a linear transformation o fthe outcome of the multi head\n",
    "\t\t# attention layer. This projects the output to the residual connection\n",
    "\t\tout = self.projection(out)\n",
    "\t\t# Add Dropout to the neural network to prevent overfitting\n",
    "\t\tout = self.dropout(out)\n",
    "\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The FeedForward Layer\n",
    "\n",
    "We will use a seperate class for performing forward passes to our module which we later will define. This is so that all tokens are feedforwarded independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents a simple linear layer followed by an activation function\n",
    "class FeedFoward(nn.Module):\n",
    "\t\n",
    "\t# Initilizes the class and creates the neural network using the\n",
    "\t# 'Sequential' model from PyTorch.\n",
    "\tdef __init__(self, n_embd):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.neural_network = nn.Sequential(\n",
    "\t\t\t# Layer 1 (input layer): linear layer, and a ReLU activation\n",
    "\t\t\tnn.Linear(n_embd, 4 * n_embd),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t# Projection layer: This layer will be use later when we introduce\n",
    "\t\t\t# resigual connections\n",
    "\t\t\tnn.Linear(4 * n_embd, n_embd),\n",
    "\t\t\t# Add Dropout to the neural network to prevent overfitting\n",
    "\t\t\tnn.Dropout(dropout),\n",
    "\t\t)\n",
    "\n",
    "\t# Forward pass through the entire network.\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.neural_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Block Class \n",
    "\n",
    "The block class helps us with scaling our neural networks since we will combine both the 'MultiHeadAttention' and the 'FeedForward' class in one single class called 'Block'. Scaling up neural networks and making them deeper comes with the problem of optimization becoming more and more difficult. To solve this issue, we introduce residual connections\n",
    "\n",
    "## Residual Connections\n",
    "### What are they?\n",
    "Residual connections act as a \"highway\" that allows the input of a layer to bypass the layer's operations (linear layer, activation functions, etc.), and be directly added to its output. This shortcut connection helps mitigate the problem of vanishing gradients in deep networks, making it easier to optimize very deep models. \n",
    "\n",
    "### Why does it work?\n",
    "By allowing the input to be directly added to the output, residual connections ensure that the network can preserve essential information and make it easier for the model to learn identity mappings, if necessary. This results in better gradient flow during backpropagation, leading to more stable and faster convergence during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents a single block of both self attention and a feed forward\n",
    "class Block(nn.Module):\n",
    "\n",
    "\tdef __init__(self, n_embd, n_head):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t# Calculate the head size\n",
    "\t\thead_size = n_embd // n_head\n",
    "\t\t# Initilize the multi head attention layer\n",
    "\t\tself.self_attention = MultiHeadAttention(n_head, head_size)\n",
    "\n",
    "\t\t# Initilize the feedforward class\n",
    "\t\tself.feedforward = FeedFoward(n_embd)\n",
    "\n",
    "\t\t# Initilie Batch Normalization Layers\n",
    "\t\tself.layer_norm_1 = nn.LayerNorm(n_embd)\n",
    "\t\tself.layer_norm_2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "\t# Forward the input data through the block of self attention and\n",
    "\t# the neural network, and add residual connections.\n",
    "\tdef forward(self, x):\n",
    "\t\t# Apply the self attention mechanism to the input tensor, 'x'.\n",
    "\t\t# Notice that we add 'x' to itself, which is the residual connection\n",
    "\t\t# we are making; the input gets added to the output. We also apply\n",
    "\t\t# batch normalization before 'x' gets applied to the self attention layer.\n",
    "\t\tx = x + self.self_attention(self.layer_norm_1(x))\n",
    "\t\t# Apply the feedforward to the updated tensor, 'x', which now includes\n",
    "\t\t# the self attention result. Notice that we again add 'x' to itself since\n",
    "\t\t# we have residual connections. We also apply\n",
    "\t\t# batch normalization before 'x' gets applied to the feedforward layer\n",
    "\t\tx = x + self.feedforward(self.layer_norm_2(x))\n",
    "\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building The Language Model\n",
    "\n",
    "### Important Note About Variable Name Convention\n",
    "\n",
    "We usually use the lables 'B', 'T', 'C', to label the dimensions of our 3D input tensors that we pass around in the neural network and I think it is important to understand what these labels actually are intuitively. \n",
    "\n",
    "* 'B' stands for \"Batch\" and represents the first dimension of our tensors and the index of the individual batches.\n",
    "* 'T' stands for \"Token\" and represents the second dimension where each elements along it corresponds to a token in the input sequence.\n",
    "* 'C' stands for \"Channel\" and represents the third dimension, which usually corresponds to the hidden size or embedding dimension, indicating the vector length of each tokenâ€™s representation in a vector space.\n",
    "\n",
    "It's super important to understand this since it is crucial that we are correct in performing numerical operations across the correct dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class representing the GPT Language Module\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "\t# Initialize the embedding table and the properties of the parent\n",
    "\t# class, 'nn.Module' (included in PyTorch for building models).\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# initilize the embedding table which is a matrix of \n",
    "\t\t# random numbers which will hold information about tokens in\n",
    "\t\t# its multidimensional vector space.\n",
    "\t\tself.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "\t\t# It is common to not only encode the indentity of the tokens but\n",
    "\t\t# also the position of them in this multidimensional embedding matrix.\n",
    "\t\t# To do this, we keep track of their positions.\n",
    "\t\tself.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\t\t# Initilize multiple blocks which include both the multihead\n",
    "\t\t# self attention and the feedforwarding.\n",
    "\t\tself.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "\t\tself.layer_norm_final = nn.LayerNorm(n_embd)\n",
    "\t\tself.model_head = nn.Linear(n_embd, vocab_size)\n",
    "\t\t\n",
    "\t# Performs a forward pass through the GPT Language Model, given a\n",
    "\t# tensor of indicies. \n",
    "\tdef forward(self, idx, targets=None):\n",
    "\t\tB, T = idx.shape\n",
    "\t\t# Look up the given index in the embedding table, this will\n",
    "\t\t# return a 3D tensor of shape (B, T, C), where 'B' is the batch\n",
    "\t\t# size, 'T' is the sequence length, and where 'C' is the channel\n",
    "\t\t# length. \n",
    "\t\ttoken_embeddings = self.token_embedding_table(idx) \n",
    "\t\ttoken_numbers = torch.arange(T, device=device) \n",
    "\t\t# Get the position embeddings of the 'token_numbers' that go from\n",
    "\t\t# 0 to T - 1, using the 'self.position_embedding_table'.\n",
    "\t\tposition_embeddings = self.position_embedding_table(token_numbers)\n",
    "\t\t# Add the identity-based, 'token_embeddings', to the position-based,\n",
    "\t\t# 'position_embeddings', to create an input matrix, 'x', that encodes\n",
    "\t\t# information about the token's identities and positions in the \n",
    "\t\t# embedding vector space. Doing this complexifies the model and\n",
    "\t\t# ultimately allows it to learn more relationships in the data,\n",
    "\t\t# leading to a better performance and generalization.\n",
    "\t\tx = token_embeddings + position_embeddings\n",
    "\t\t# Feed forward the data through the defined 'self.blocks' to \n",
    "\t\t# apply the multi head attention layer and feed the data\n",
    "\t\t# through the neural network.\n",
    "\t\tx = self.blocks(x)\n",
    "\t\tx = self.layer_norm_final(x)\n",
    "\t\t# Pass the input data through the linear layer to output 'logits'.\n",
    "\t\tlogits = self.model_head(x)\n",
    "\n",
    "\t\tif targets is None:\n",
    "\t\t\tloss = None\n",
    "\t\telse:\n",
    "\t\t\t# We know want to view the 'logits' not as a 3D tensor but\n",
    "\t\t\t# a 2D tensor, which we do by changing the first dimension\n",
    "\t\t\t# to 'B * T', while keeping the size of the channel length.\n",
    "\t\t\t# Essentially, we are stretching the array to a 2D array so\n",
    "\t\t\t# that we later can perform numerical operations with it. #\n",
    "\t\t\t# By default, we usually epect our data to be 2D.\n",
    "\t\t\tB, T, C = logits.shape\n",
    "\t\t\tlogits = logits.view(B * T, C)\n",
    "\n",
    "\t\t\t# We now need to do the same to the 'targets' since they currently\n",
    "\t\t\t# are of shape (B, T), a 2D array which we want to turn into 1D.\n",
    "\t\t\ttargets = targets.view(B * T)\n",
    "\n",
    "\t\t\t# Calculate the loss \n",
    "\t\t\tloss = F.cross_entropy(logits, targets)\n",
    "\n",
    "\t\treturn logits, loss\n",
    "\n",
    "\t# Generates a set amount of new tokens, 'max_new_tokens' (similar to words), \n",
    "\t# given the most recent probabilities or 'logits'.\n",
    "\tdef generate(self, idx, max_new_tokens):\n",
    "\t\tfor _ in range(max_new_tokens):\t\t\t\n",
    "\t\t\t# Crop the 'idx' to the last 'block_size' tokens so that the indexing\n",
    "\t\t\t# doesn't run out of bounds. Since our positional embeddings has \n",
    "\t\t\t# an embedding dimension of 'block_size', we can't index past this.\n",
    "\t\t\tidx_cropped = idx[:, -block_size:]\n",
    "\n",
    "\t\t\t# get the predictions\n",
    "\t\t\tlogits, loss = self(idx_cropped)\n",
    "\n",
    "\t\t\t# Index the logits to get only the recent logits\n",
    "\t\t\tlogits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "\t\t\t# apply softmax to get probabilities\n",
    "\t\t\tprobs = F.softmax(logits, dim=-1) # (B, C)\n",
    "\n",
    "\t\t\t# sample from the distribution\n",
    "\t\t\tidx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "\t\t\t# append sampled index to the running sequence \n",
    "\t\t\tidx = torch.cat((idx, idx_next), dim=1) # (B, T + 1)\n",
    "\t\t\t\n",
    "\t\treturn  idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before training: 4.360526084899902\n",
      "Output before training: \n",
      "m-RZ-NJzt3DGPKKhyUrWbQiRTtd  M.g-IcYMR'\n",
      " iMLt'fDAwitsXOO;l!ODmLI??h?vdFLCEGy Uhac33k-;FtYRdypc:j3Jig\n"
     ]
    }
   ],
   "source": [
    "# Initilize the model and set it so that the model will be traind on ur GPU\n",
    "model = GPTLanguageModel()\n",
    "model_gpu = model.to(device) \n",
    "\n",
    "token_embeddings, loss = model_gpu(x_batch_train, y_batch_train)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_tokens = model_gpu.generate(idx=idx, max_new_tokens=100)[0].tolist()\n",
    "decoded_tokens = decode(generated_tokens)\n",
    "\n",
    "print('Loss before training:', loss.item())\n",
    "print('Output before training:', decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/   5000: 4.3499\n",
      "    250/   5000: 2.3959\n",
      "    500/   5000: 2.1050\n",
      "    750/   5000: 1.8632\n",
      "   1000/   5000: 1.6848\n",
      "   1250/   5000: 1.6127\n",
      "   1500/   5000: 1.5589\n",
      "   1750/   5000: 1.4631\n",
      "   2000/   5000: 1.4147\n",
      "   2250/   5000: 1.4049\n",
      "   2500/   5000: 1.3645\n",
      "   2750/   5000: 1.3120\n",
      "   3000/   5000: 1.2882\n",
      "   3250/   5000: 1.3301\n",
      "   3500/   5000: 1.2251\n",
      "   3750/   5000: 1.2289\n",
      "   4000/   5000: 1.2528\n",
      "   4250/   5000: 1.2302\n",
      "   4500/   5000: 1.1938\n",
      "   4750/   5000: 1.2062\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(max_iters): \n",
    "\n",
    "\t# sample a batch of data\n",
    "\txb, yb = get_batch('train')\n",
    "\n",
    "\t# evaluate the loss\n",
    "\ttoken_embeddings, loss = model_gpu(xb, yb)\n",
    "\n",
    "\t# Reset the gradients\n",
    "\toptimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "\t# Backpropagate\n",
    "\tloss.backward()\n",
    "\n",
    "\t# AdamW Optimization\n",
    "\toptimizer.step()\n",
    "\n",
    "\t# track stats\n",
    "\tif i % 250 == 0: \n",
    "\t\tprint(f'{i:7d}/{max_iters:7d}: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: gpt_saved_models\\gpt_v1.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "SAVES_BASE_PATH = Path(\"gpt_saved_models\")\n",
    "SAVES_BASE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_FILE_NAME = f\"{model_name}.pth\"\n",
    "MODEL_SAVE_PATH = SAVES_BASE_PATH / MODEL_FILE_NAME\n",
    "\n",
    "SAVE_OBJECT = model.state_dict()\n",
    "\n",
    "torch.save(obj=SAVE_OBJECT, f=MODEL_SAVE_PATH) \n",
    "print(f\"Saved model to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 500\n",
    "\n",
    "# This function both makes the forward pass through the model and\n",
    "# prints the loss and generated text output, given some input tensors\n",
    "# 'x', and 'y'\n",
    "def evaluate(x, y):\n",
    "\t# Forward pass through the model: Get the token_embeddings, and\n",
    "\t# the loss\n",
    "\ttoken_embeddings, loss = model(x, y)\n",
    "\n",
    "\t# Get the index??\n",
    "\tidx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\t# Use the model to generate new tokesn up until 'max_tokens'\n",
    "\tgenerated_tokens = model_gpu.generate(idx=idx, max_new_tokens=max_tokens)[0].tolist()\n",
    "\t# Decode tokens from numerical data to categorical, text data.\n",
    "\tdecoded_tokens = decode(generated_tokens)\n",
    "\n",
    "\t# Print the 'loss' and the 'decoded_tokens'\n",
    "\tprint('Loss after training:', loss.item())\n",
    "\tprint('Output after training:', decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation After Training For Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation On Training Dataset:\n",
      "\n",
      "\n",
      "Loss after training: 1.1423503160476685\n",
      "Output after training: \n",
      "O me, More sides,--or for the abegainst our open,\n",
      "Ay, may justice, win wait,--it is a little plent remore,\n",
      "Or your honour's est secret in: pray you, but a\n",
      "power visating a doit or poor execute, to\n",
      "work so true Haptis.\n",
      "\n",
      "MENENIUS:\n",
      "Worthy were! I know me, by thine own tongue!\n",
      "\n",
      "COMINIUS:\n",
      "Even abide men an obsedause colour.\n",
      "I do your ladys.\n",
      "\n",
      "A PETRUS:\n",
      "My soul heart sir, I will shall tell so; then it did be\n",
      "With wastestice as 'tis not, that was worb a face;\n",
      "Take thou fly talk of my fault, us else ther\n"
     ]
    }
   ],
   "source": [
    "print('Model Evaluation On Training Dataset:\\n\\n')\n",
    "evaluate(x_batch_train, y_batch_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation After Training For Val Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation On Validation Dataset:\n",
      "\n",
      "\n",
      "Loss after training: 1.4951786994934082\n",
      "Output after training: \n",
      "\n",
      "After, gindening you good,\n",
      "if I shall show it be, sir, and so re truth,\n",
      "'Tis it was make landly open before him.'\n",
      "\n",
      "GLOUCESTER:\n",
      "I have, my lady's your grace's ere then past.\n",
      "\n",
      "HASTINGS:\n",
      "My lord, where is your lord? and never good\n",
      "In your honour tears wear of love chancely: for myself,\n",
      "Go, my lord and look, now you our interpent\n",
      "Of man's ignorance! who's this title Musi was signiobly!\n",
      "You best ror an cold dove and cut for then?\n",
      "Before, thy life, sir; then comes too his quare more.\n",
      "For past to spea\n"
     ]
    }
   ],
   "source": [
    "print('Model Evaluation On Validation Dataset:\\n\\n')\n",
    "evaluate(x_batch_val, y_batch_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
